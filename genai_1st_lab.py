# -*- coding: utf-8 -*-
"""GenAi 1st Lab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aZ_0AlCm7qSwrwli_5sECWyxMRV4pJUI
"""

!pip install transformers

from transformers import pipeline

classifier = pipeline("sentiment-analysis")

res=classifier( ' i am good but feeling tired')
print(res)

classifier = pipeline(task='text-generation', model='gpt2')
res=classifier('i am good but feeling tired')
print(res)

geberator = pipeline('text-generation', model='distilgpt2')
res=geberator('i am good but feeling tired', max_length=30, num_return_sequences=2)
print(res)

classifier = pipeline(task='zero-shot-classification')
res=classifier('I am excited about the new project but anxious about deadlines.', candidate_labels=['positive', 'negative'])
print(res)

from transformers import MBartForConditionalGeneration, MBartTokenizer

# Define the source and target languages
src_lang = "en_XX"  # English
tgt_lang = "hi_IN"  # Hindi

# Load the tokenizer and model
model_name = "facebook/mbart-large-50-many-to-many-mmt"
tokenizer = MBartTokenizer.from_pretrained(model_name)
model = MBartForConditionalGeneration.from_pretrained(model_name)

# Define the text to be translated
text = "Learning new tech and programming skills can help your career."

# Prepare the input for the model
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
tokenizer.src_lang = src_lang  # Set source language
translated = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang])

# Decode the translated text
translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)

print(f"Original text: {text}")
print(f"Translated text: {translated_text}")

from transformers import pipeline

# Initialize the summarization pipeline
summarizer = pipeline("summarization")

# Sample text to summarize
text = """
Machine learning (ML) is a field of artificial intelligence (AI) that uses algorithms to learn from and make predictions or decisions based on data. Machine learning has been widely used in various applications, from spam filtering to medical diagnostics. One of the key aspects of machine learning is its ability to improve over time as it is exposed to more data. In recent years, machine learning techniques have advanced significantly, thanks to improvements in computing power and the availability of large datasets.
"""

# Generate a summary
summary = summarizer(text, max_length=50, min_length=25, do_sample=False)

# Print the summary
print("Summary:", summary[0]['summary_text'])

from transformers import BartForConditionalGeneration, BartTokenizer

# Load the tokenizer and model for summarization
model_name = "facebook/bart-large-cnn"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

# Define the text to be summarized
text = """
vibrant street markets, lies a small, serene park that offers a peaceful retreat from the urban chaos. The park, with its lush greenery and tranquil pond, serves as a sanctuary for both locals and visitors seeking respite from their busy lives. On any given day, you might find people relaxing on benches, children playing on the playground, or joggers taking in the fresh air. The gentle rustle of leaves and the soft chirping of birds create a soothing backdrop, making it a perfect spot for reflection and relaxation amidst the city's hustle and bustle.
"""

# Tokenize the input text
inputs = tokenizer(text, return_tensors="pt", max_length=1024, truncation=True)

# Generate the summary
summary_ids = model.generate(inputs["input_ids"], max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)

# Decode the summary
summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print(f"Original text: {text}")
print(f"Summary: {summary_text}")

from transformers import pipeline

# Load the question-answering pipeline
qa_pipeline = pipeline("question-answering")

# Define the context and question
context = """
Hugging Face is a company specializing in Natural Language Processing (NLP). They provide an extensive library of pre-trained models for a variety of NLP tasks, including text generation, translation, summarization, and more. Their open-source library, transformers, allows developers to easily integrate these models into their own applications. With a focus on state-of-the-art performance and ease of use, Hugging Face has become a popular tool in the AI and machine learning communities.
"""

question = "What is Hugging Face known for?"

# Use the pipeline to get the answer
result = qa_pipeline(question=question, context=context)

# Print the result
print(f"Question: {question}")
print(f"Answer: {result['answer']}")

# Install necessary libraries
!pip install transformers

from transformers import pipeline

# Sentiment Analysis
# Analyzes the sentiment of the given text
classifier = pipeline("sentiment-analysis")
res = classifier('I am good but feeling tired')
print(res)

# Text Generation using GPT-2
# Generates text based on the input seed text
classifier = pipeline(task='text-generation', model='gpt2')
res = classifier('I am good but feeling tired')
print(res)

# Text Generation using DistilGPT-2
# Generates multiple sequences based on the input text
generator = pipeline('text-generation', model='distilgpt2')
res = generator('I am good but feeling tired', max_length=30, num_return_sequences=2)
print(res)

# Zero-Shot Classification
# Classifies text into predefined categories without any prior training
classifier = pipeline(task='zero-shot-classification')
res = classifier(
    'I am excited about the new project but anxious about deadlines.',
    candidate_labels=['positive', 'negative']
)
print(res)

# Translation using MBart
from transformers import MBartForConditionalGeneration, MBartTokenizer

# Set source and target languages
src_lang = "en_XX"  # English
tgt_lang = "hi_IN"  # Hindi

# Load tokenizer and model
model_name = "facebook/mbart-large-50-many-to-many-mmt"
tokenizer = MBartTokenizer.from_pretrained(model_name)
model = MBartForConditionalGeneration.from_pretrained(model_name)

# Text to be translated
text = "Learning new tech and programming skills can help your career."

# Prepare input, translate, and decode
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
translated = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang])
translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)

print(f"Original text: {text}")
print(f"Translated text: {translated_text}")

# Summarization
from transformers import pipeline

# Initialize the summarization pipeline
summarizer = pipeline("summarization")

# Sample text to summarize
text = """
Exploring new technologies often requires a deep understanding of both theoretical concepts and practical applications. Embracing this learning process can significantly enhance problem-solving skills and open up diverse career opportunities.
"""

# Summarize the text
summary = summarizer(text, max_length=30, min_length=10, do_sample=False)
print(f"Original text: {text}")
print(f"Summary: {summary[0]['summary_text']}")


# Question-Answering
# Uses a context and a question to provide an answer
qa_pipeline = pipeline("question-answering")

# Define context and question
context = """
Hugging Face is a company specializing in Natural Language Processing (NLP). They provide an extensive library of pre-trained models for a variety of NLP tasks, including text generation, translation, summarization, and more.
"""
question = "What is Hugging Face known for?"

# Get and print the answer
result = qa_pipeline(question=question, context=context)
print(f"Question: {question}")
print(f"Answer: {result['answer']}")

# Summarization
from transformers import pipeline

# Initialize the summarization pipeline
summarizer = pipeline("summarization")

# Sample text to summarize
text = """
Exploring new technologies often requires a deep understanding of both theoretical concepts and practical applications. Embracing this learning process can significantly enhance problem-solving skills and open up diverse career opportunities.
"""

# Summarize the text
summary = summarizer(text, max_length=30, min_length=10, do_sample=False)
print(f"Original text: {text}")
print(f"Summary: {summary[0]['summary_text']}")