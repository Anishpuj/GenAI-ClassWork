# -*- coding: utf-8 -*-
"""Genai 2nd Lab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mkaX5TuihEW7vnUI0ftYwUyrF04flz56

#Class Example
"""

# -*- coding: utf-8 -*-
"""
LSTM Text Prediction

This script demonstrates how to build and train an LSTM model for text prediction.
"""

# Install TensorFlow (which includes Keras)
!pip install tensorflow

# Import the necessary libraries
import numpy as np
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import LSTM, Dense, Embedding, Reshape
from tensorflow.keras.models import Sequential

# Source input text for training the model
inputdata = """
Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning.
Unlike standard feedforward neural networks, LSTM has feedback connections. It can process not only single data points (such as images),
but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected
handwriting recognition, speech recognition, and anomaly detection in network traffic or IDSs (intrusion detection systems).
A common LSTM unit is composed of a cell, an input gate, an output gate, and a forget gate. The cell remembers values over arbitrary
time intervals and the three gates regulate the flow of information into and out of the cell.
LSTM networks are well-suited to classifying, processing, and making predictions based on time series data, since there can be lags
of unknown duration between important events in a time series. LSTMs were developed to deal with the vanishing gradient problem that
can be encountered when training traditional RNNs. Relative insensitivity to gap length is an advantage of LSTM over RNNs, hidden
Markov models, and other sequence learning methods in numerous applications.
"""

# Step 1: Integer encoding the text
# Tokenizer is used to convert words into their respective integer indices
tokenizer = Tokenizer()
tokenizer.fit_on_texts([inputdata])
encoded_data = tokenizer.texts_to_sequences([inputdata])[0]
print(encoded_data)  # View the encoded data

# Step 2: Determine the vocabulary size
# The vocabulary size is the total number of unique words in the text
vocab_size = len(tokenizer.word_index) + 1
print(vocab_size)  # Display vocabulary size

# Step 3: Create sequences of words to fit the model
# Sequences are created by taking pairs of consecutive words
sequences = []
for i in range(1, len(encoded_data)):
    sequence = encoded_data[i-1:i+1]
    sequences.append(sequence)
print(sequences)  # View the sequences

# Step 4: Prepare the data for training
# The sequences are split into input (x) and output (y) components
sequences = np.array(sequences)
x, y = sequences[:, 0], sequences[:, 1]

# Step 5: One-hot encode the output labels
# The output labels (y) are one-hot encoded
y = to_categorical(y, num_classes=vocab_size)
print(y[:5])  # View the first 5 encoded labels

# Step 6: Define the LSTM model
# The model uses an embedding layer, an LSTM layer, and a dense output layer
model = Sequential()
model.add(Embedding(vocab_size, 10, input_length=1))
model.add(LSTM(50))
model.add(Dense(vocab_size, activation='softmax'))
model.summary()  # Display the model's architecture

# Step 7: Compile the model
# The model is compiled using categorical crossentropy as the loss function and Adam as the optimizer
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# (Alternative) Define the LSTM model with Reshape layer
# This alternative model uses a Reshape layer to adjust the input shape for the LSTM layer
model = Sequential()
model.add(Embedding(vocab_size, 10, input_length=1))
model.add(Reshape((-1, 10)))  # Reshape to (batch_size, time_steps, features)
model.add(LSTM(50))
model.add(Dense(vocab_size, activation='softmax'))
model.summary()  # Display the model's architecture

# Step 8: Define a function to generate sequences of words
# The function generates a sequence of words based on the trained model and a given seed word
def generate_seq(model, tokenizer, enter_text, n_pred):
    in_text, result = enter_text, enter_text
    for i in range(n_pred):
        encoded = tokenizer.texts_to_sequences([in_text])[0]
        encoded = np.array(encoded)
        y_prob = model.predict(encoded)
        yhat = y_prob.argmax(axis=-1)
        out_word = ''
        for word, index in tokenizer.word_index.items():
            if index == yhat:
                out_word = word
                break
        in_text, result = out_word, result + ' ' + out_word
    return result

# Step 9: Generate and print a sequence of words
# The model predicts the next 7 words based on the seed word 'lstm'
print(generate_seq(model, tokenizer, 'lstm', 2))

model.summary()

"""#Class Example #2"""

# -*- coding: utf-8 -*-
"""
LSTM Text Prediction

This script demonstrates how to build and train an LSTM model for text prediction.
"""

# Install TensorFlow (which includes Keras)
!pip install tensorflow

# Import the necessary libraries
import numpy as np
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import LSTM, Dense, Embedding, Reshape
from tensorflow.keras.models import Sequential

# Source input text for training the model
inputdata = """
Climate change refers to long-term shifts and alterations in temperature and weather patterns, primarily due to human activities such as burning fossil fuels.
These changes have far-reaching consequences, affecting ecosystems, weather patterns, and even human health.
One of the most significant effects of climate change is the increase in global temperatures, which leads to the melting of polar ice caps and rising sea levels.
This, in turn, causes flooding in coastal areas and the displacement of communities.
Moreover, climate change contributes to the frequency and intensity of extreme weather events, such as hurricanes, droughts, and wildfires.
Efforts to mitigate climate change include reducing carbon emissions, transitioning to renewable energy sources, and conserving natural habitats.
The global community must work together to address climate change and protect the planet for future generations.
"""

# Step 1: Integer encoding the text
# Tokenizer is used to convert words into their respective integer indices
tokenizer = Tokenizer()
tokenizer.fit_on_texts([inputdata])
encoded_data = tokenizer.texts_to_sequences([inputdata])[0]
print(encoded_data)  # View the encoded data

# Step 2: Determine the vocabulary size
# The vocabulary size is the total number of unique words in the text
vocab_size = len(tokenizer.word_index) + 1
print(vocab_size)  # Display vocabulary size

# Step 3: Create sequences of words to fit the model
# Sequences are created by taking pairs of consecutive words
sequences = []
for i in range(1, len(encoded_data)):
    sequence = encoded_data[i-1:i+1]
    sequences.append(sequence)
print(sequences)  # View the sequences

# Step 4: Prepare the data for training
# The sequences are split into input (x) and output (y) components
sequences = np.array(sequences)
x, y = sequences[:, 0], sequences[:, 1]

# Step 5: One-hot encode the output labels
# The output labels (y) are one-hot encoded
y = to_categorical(y, num_classes=vocab_size)
print(y[:5])  # View the first 5 encoded labels

# Step 6: Define the LSTM model
# The model uses an embedding layer, an LSTM layer, and a dense output layer
model = Sequential()
model.add(Embedding(vocab_size, 10, input_length=1))
model.add(LSTM(50))
model.add(Dense(vocab_size, activation='softmax'))
model.summary()  # Display the model's architecture

# Step 7: Compile the model
# The model is compiled using categorical crossentropy as the loss function and Adam as the optimizer
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# (Alternative) Define the LSTM model with Reshape layer
# This alternative model uses a Reshape layer to adjust the input shape for the LSTM layer
model = Sequential()
model.add(Embedding(vocab_size, 10, input_length=1))
model.add(Reshape((-1, 10)))  # Reshape to (batch_size, time_steps, features)
model.add(LSTM(50))
model.add(Dense(vocab_size, activation='softmax'))
model.summary()  # Display the model's architecture

# Step 8: Define a function to generate sequences of words
# The function generates a sequence of words based on the trained model and a given seed word
def generate_seq(model, tokenizer, enter_text, n_pred):
    in_text, result = enter_text, enter_text
    for i in range(n_pred):
        encoded = tokenizer.texts_to_sequences([in_text])[0]
        encoded = np.array(encoded)
        y_prob = model.predict(encoded)
        yhat = y_prob.argmax(axis=-1)
        out_word = ''
        for word, index in tokenizer.word_index.items():
            if index == yhat:
                out_word = word
                break
        in_text, result = out_word, result + ' ' + out_word
    return result

# Step 9: Generate and print a sequence of words
# The model predicts the next 7 words based on the seed word 'climate'
print(generate_seq(model, tokenizer, 'climate', 7))

model.summary()

# Import necessary libraries
import numpy as np
from numpy import array
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import LSTM, Dense, Embedding
from tensorflow.keras.models import Sequential
import matplotlib.pyplot as plt

# Source input text
inputdata = """Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can process not only single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition and anomaly detection in network traffic or IDSs (intrusion detection systems). A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell. LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the vanishing gradient problem that can be encountered when training traditional RNNs. Relative insensitivity to gap length is an advantage of LSTM over RNNs, hidden Markov models and other sequence learning methods in numerous applications."""

# Integer encoding
tokenizer = Tokenizer()
tokenizer.fit_on_texts([inputdata])
encoded_data = tokenizer.texts_to_sequences([inputdata])[0]

# Create sequences
sequences = list()
for i in range(1, len(encoded_data)):
    sequence = encoded_data[i-1:i+1]
    sequences.append(sequence)

# Convert sequences to array and split into x and y
sequences = array(sequences)
x, y = sequences[:, 0], sequences[:, 1]

# One-hot encode y
vocab_size = len(tokenizer.word_index) + 1
y = to_categorical(y, num_classes=vocab_size)

# Reshape x to match LSTM input shape (num_samples, time_steps, features)
x = x.reshape((x.shape[0], 1))  # Reshape x to (num_samples, time_steps)

# Build the model
model = Sequential()
model.add(Embedding(vocab_size, 10, input_length=1))
model.add(LSTM(50))
model.add(Dense(vocab_size, activation='softmax'))
model.summary()

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(x, y, epochs=100, verbose=1)

# Plot training accuracy
plt.plot(history.history['accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.show()

# Function to generate sequences
def generate_seq(model, tokenizer, enter_text, n_pred):
    in_text, result = enter_text, enter_text
    for _ in range(n_pred):
        encoded = tokenizer.texts_to_sequences([in_text])[0]
        encoded = array(encoded)
        encoded = encoded.reshape(1, -1)  # Reshape for prediction
        y_prob = model.predict(encoded, verbose=0)
        yhat = y_prob.argmax(axis=-1)
        out_word = ''
        for word, index in tokenizer.word_index.items():
            if index == yhat:
                out_word = word
                break
        in_text, result = out_word, result + ' ' + out_word
    return result

# Generate text
print(generate_seq(model, tokenizer, 'lstm', 7))